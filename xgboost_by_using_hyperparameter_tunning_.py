# -*- coding: utf-8 -*-
"""XGboost by using hyperparameter tunning .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OTIMeVWjBBiRTeOyYrmKhMNomV41Um7g
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost   as xgb
import optuna # is used to perform the hyperparameter tunning ( it is way faster than GridSearchCV and RandomSeacrhCV)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
# optuna

pip install optuna

df=pd.read_csv("https://raw.githubusercontent.com/srinivasav22/Graduate-Admission-Prediction/master/Admission_Predict_Ver1.1.csv")

df.head()

df.isnull().sum()

df.duplicated()

df.shape

df.drop_duplicates()

df.isnull().sum()

df.info()

X=df.drop(['Serial No.','Chance of Admit '],axis=1)

y=df["Chance of Admit "]

X.shape

y.shape

X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.25,random_state=45)

X_train.shape

std_scaler=StandardScaler()

X_train=std_scaler.fit_transform(X_train)

X_train.shape

X_test=std_scaler.transform(X_test)

X_test.shape

# to used optuna we have to create a objective function and then pass the parameter to the model and then  you will get the best_paramters

def objective(trial, data=X, target=y):
    X_train, X_test, Y_train, y_test = train_test_split(data, target, test_size=0.25, random_state=45)
    params = {
        "n_estimators": 3000,
        "max_depth": trial.suggest_categorical("max_depth", [3, 4, 5, 6, 7, 8, 9, 10]),
        "random_state": trial.suggest_categorical("random_state", [10, 20, 30, 2000, 2454, 24533]),
        "learning_rate": trial.suggest_categorical("learning_rate", [0.001, 0.00001, 0.08, 0.09, 0.01, 0.2]),
        "tree_method": "gpu_hist",
        "subsample": trial.suggest_categorical("subsample", [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 200),
        "colsample_bytree": trial.suggest_categorical("colsample_bytree", [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),
        "reg_alpha": trial.suggest_loguniform("reg_alpha", 1e-4, 10.0),
        "reg_lambda": trial.suggest_loguniform('reg_lambda', 1e-4, 10.0)
    }

    X_gb = xgb.XGBRegressor(**params)
    X_gb.fit(X_train, Y_train, eval_set=[(X_test, y_test)], verbose=True)
    y_pred = X_gb.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    return mse

cr_study=optuna.create_study()
cr_study.optimize(objective,n_trials=6)
cr_study.best_trail.params

!nvidia-smi



